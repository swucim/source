title: 中国名医琅琊榜数据分析1-数据爬取
date: 2015-12-06 14:22:06
tags: [可视化案例, 爬虫]
categories: Python
---
# 网络爬虫的实现
## 确定目标
近日，网络上有一份2014中国名医“琅琊榜”发布，据消息称这是一位南京从医十几年的外科医生，调研全国200家医院，查询了3000名医生的专长、手术量、科研成果后制作的。他将11个科室细分为62个专业，每个专业筛选出顶尖高手，最终制作出《中国名医琅琊榜》。那么，何不对这份榜单进行一下数据分析呢？

数据分析的第一步是获取数据，根据资料很快锁定[信息来源](http://www.top10dr.com),分析网站，该网站分别从年度、区域、疾病类型、手术类型四个维度提供了所属医生具体信息查询，那么可以大概确定要抓取的数据模型如下：
![](http://7xoxf6.com1.z0.glb.clouddn.com/top10drmodel.png)
## 准备工作
### 确定框架
相较于动态网页，静态网页处理起来要容易的多，不过既然网络上有这么多爬虫框架，何不拿来一用呢？在此我选择Scrapy框架，网络上有很多现成的简单示例，也可以直接参考官方文档里面提供[示例教程](http://scrapy-chs.readthedocs.org/zh_CN/latest/intro/tutorial.html)，对于入门是足够了。

### 开发环境搭建
安装Scrapy及相关包的过程在此略过，在开发过程中使用到的开发工具依次有：
<br>PyCharm (Python IDE)
<br>MySQL and Navicat (DB and DBMS)
<br>Chrome浏览器 (使用审查元素快速定位元素xpath位置,也可以选择使用xpath扩展程序)

### 大致步骤
简单说来，实现一个Scrapy爬虫分为四步走：
<br>1. 创建一个Scrapy项目
<br>2. 定义提取的Item
<br>3. 编写爬取网站的 spider 并提取 Item
<br>4. 编写 Item Pipeline 来存储提取到的Item(即数据)

## 开始编写
### 创建项目
选择一个本地目录，在CMD中运行以下命令：<br>
```python
scrapy startproject top10dr
```
该命令会自动生成名为top10dr的项目文件夹，并且包含以下项目文件：
<br>scrapy.cfg: 项目的配置文件
<br>top10dr/: 该项目的python模块, 在此加入代码。
<br>top10dr/items.py: 项目中的item文件.
<br>top10dr/pipelines.py: 项目中的pipelines文件.
<br>top10dr/settings.py: 项目的设置文件.
<br>top10dr/spiders/: 放置spider代码的目录.
### 定义Item
以Area_D为例, items.py文件如下：
```python
import scrapy
class AreaItem(scrapy.Item):
    hospital = scrapy.Field() //hospital_name
    area = scrapy.Field()
    name = scrapy.Field() //doctor_name
    link = scrapy.Field() //doctor_link for further usage of doctor details crawling.
```
与此对应，在数据库中也建立相应表和字段。
### 编写Spider
分析[地区页面](http://www.top10dr.com/diqumy.html)下各地区链接特点，从北京市开始 www.top10dr.com/diqu/1.html，最终的html文件仅文件名依次递增，前面链接完全相同。因此start_urls可以像这样定义(有部分省市没有上榜数据，不过没有关系，不影响我们爬取的结果)：
```python
start_urls = []
    for i in range(1,35):
        start_urls.append("http://www.top10dr.com/diqu/%d.html"%(i))
```
进一步分析每个地区下的页面元素
![](http://7xoxf6.com1.z0.glb.clouddn.com/datavizdemoareaxpath.png)
首先获得列表公共xpath部分
```python
sites = sel.xpath('//div[@class="searchwrap"]/div[@class="searchss"]')
```
然后对sites下的每个元素继续解析xpath得到医生姓名、所在医院、医生详细链接等信息，
```python
for site in sites:
	link = site.xpath('a/@href').extract()
	name = site.xpath('a/img/@alt').extract()
	hospital = site.xpath('p/strong/text()').extract()
```
最后把这些item一起返回，交给下一步pipLine处理，top10dr_spider.py部分完整代码如下：
```python
class AreaSpider(scrapy.Spider):
    name = "area"
    allowed_domains = ["top10dr.com"]
    start_urls = []
    for i in range(1,35):
        start_urls.append("http://www.top10dr.com/diqu/%d.html"%(i))

    def parse(self, response):
        sel = Selector(response)
        sites = sel.xpath('//div[@class="searchwrap"]/div[@class="searchss"]')
        diqu = sel.xpath('//div[@class="diquwrap"]/h5/span/text()').extract()
        items = []
        url = 'http://top10dr.com'

        for site in sites:
            item = AreaItem()
            link = site.xpath('a/@href').extract()
            name = site.xpath('a/img/@alt').extract()
            hospital = site.xpath('p/strong/text()').extract()
            item['link'] = [url + l.encode('utf-8') for l in link]
            item['hospital'] = [h.encode('utf-8') for h in hospital]
            item['name'] = [n.encode('utf-8') for n in name]
            item['area'] = [d.encode('utf-8') for d in diqu]
            items.append(item)
            logging.log(logging.INFO,'Appeding area item...')
        logging.log(logging.INFO,'Appeding area Done!')
        return items
```
### 编写Pipline
我们准备将返回的结果保存到数据库里面，因此要在pipline中建立数据库连接，不要忘了在创建数据库连接的参数中指定编码方式**charset='utf-8'**, 不然会导致存入到数据库中的中文产生乱码。

```python
from scrapy import log
from twisted.enterprise import adbapi
import MySQLdb
import MySQLdb.cursors
import logging
class AreaPipeline(object):
    def __init__(self):
        self.dbpool = adbapi.ConnectionPool('MySQLdb',db = 'top10dr',user = 'root',passwd = 'admin',cursorclass = MySQLdb.cursors.DictCursor,charset = 'utf8',use_unicode = False)
    def handle_error(self,e):
        logging.log(logging.INFO,'error!.')
    def process_item(self, item, spider):
        if spider.name not in ['area']:
            return item
        else:
            query = self.dbpool.runInteraction(self._conditional_insert, item)
            query.addErrback(self.handle_error)
            return item

    def _conditional_insert(self,tx,item):
        result=False
        if result:
            log.msg("Item already stored in db:%s" % item,level=log.DEBUG)
        else:
            # print 'start insert'
            tx.execute("insert into AREA_D(AREA_NM,HSP_NM,DR_LK,DR_NM) values(%s,%s,%s,%s);",
              (item['area'][0],
               item['hospital'][0],
               item['link'][0],
               item['name'][0]))
```
### 即将完成
在测试之前还需要做一件事情，就是在settings.py文件中注册刚刚创建的ITEM_PIPLINES
```python
ITEM_PIPELINES = {
    'top10dr.pipelines.AreaPipeline': 300
｝
```
接下来, 就可以进入工程的根目录，在cmd中敲入命令：
```python
scrapy crawl area
```
来测试程序运行结果了！
如果需要查看log信息调试程序，也可以在命令行后面指定logfile参数：
```python
scrapy crawl area --logfile area.log
```
最终，我们就得到了数据库中这样一张表:

![](http://7xoxf6.com1.z0.glb.clouddn.com/top10drdb.png)

其他时间、疾病类型、手术类型等维度数据的爬取过程大体和地区维度相类似，都是简单的三板斧：
1. 定义item，
2. 编写spider，
3. 实现pipline

好了，如果你已经看到了这里，相信已经了解数据获取是怎么实现的了，赶紧搭建一个环境去实现吧。如果在实现的过程中有任何问题，欢迎在评论中留言。

[源代码](https://github.com/swucim/top10dr)
